{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Envoiriment for the  functions in the module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How To Use Conda Environment In a Jupyter Notebook\n",
    "](https://saturncloud.io/blog/how-to-use-conda-environment-in-a-jupyter-notebook/#:~:text=Using%20a%20Conda%20Environment%20in%20Jupyter%20Notebook&text=This%20package%20allows%20you%20to,be%20used%20in%20Jupyter%20Notebook.&text=This%20will%20create%20a%20new%20kernel%20for%20the%20“myenv”%20environment,be%20used%20in%20Jupyter%20Notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python main.py --model --datasetpath --state \"train\" --pretrained_path --output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *\n",
    "from Metrics import *\n",
    "from torch.nn.functional import mse_loss\n",
    "from kornia.losses import ssim_loss\n",
    "from numpy import mean \n",
    "from time import time\n",
    "\n",
    "class ssim_1(nn.Module):\n",
    "    def __init__(self, window_size):\n",
    "        super(ssim_1,self).__init__()\n",
    "        self.window_size = window_size\n",
    "    def forward(self, input, target):\n",
    "        return ssim_loss(input, target, self.window_size)\n",
    "\n",
    "class mse_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mse_1,self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return mse_loss(input, target)\n",
    "\n",
    "gt_image = torch.randn(1, 3, 64, 64, requires_grad=True)\n",
    "t_image = torch.randn(1, 3, 64, 64, requires_grad=True)\n",
    "mean_val = mean(t_image.detach().numpy())\n",
    "\n",
    "# Example usage of the Dark Channel Prior loss function\n",
    "#find patch size\n",
    "patch_size = 5\n",
    "print(mean)\n",
    "losses = {\n",
    "    \"DarkChannelPriorLoss\": DarkChannelPriorLoss(patch_size),\n",
    "    \"L_exp\": L_exp(patch_size,mean_val),\n",
    "    \"L_color\": L_color(),\n",
    "    \"mse_loss\":mse_1(),\n",
    "    \"light_loss\": light_loss2(),\n",
    "    \"color_loss\": color_loss2(),\n",
    "    \"L1Loss\": L1Loss(),\n",
    "    \"SSIM loss\":ssim_1(window_size=11),\n",
    "    \"lch_loss\": lch_Loss(),\n",
    "    \"lab_loss\": lab_Loss(),\n",
    "    \"loss_fn_alex\" : LPIPS(net='alex'), # best forward scores\n",
    "    \"loss_fn_vgg\" : LPIPS(net='vgg'), # closer to \"traditional\" perceptual loss, when used for optimization\n",
    "    \"loss_squeeze\" : LPIPS(net='squeeze') # lightest, faster, narrower, lower quality\n",
    "}\n",
    "\n",
    "for name, loss in losses.items():\n",
    "    start_time = time()\n",
    "    if name== \"loss_fn_vgg\" or name == \"loss_squeeze\" or name == \"loss_fn_alex\" or name == \"L1Loss\" or name == \"light_loss\" or name == \"color_loss\" or name == \"SSIM loss\" or name == \"color_loss\" or name == \"mse_loss\" or name == \"lab_loss\" or name == \"lch_loss\":\n",
    "        loss = loss(gt_image, t_image)\n",
    "    else:\n",
    "        loss = loss(t_image)\n",
    "    end_time = time()\n",
    "    print(name, \":\", loss.item(), \"// Time:\", end_time-start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting colorspacious\n",
      "  Downloading colorspacious-1.1.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /home/gusanagy/anaconda3/envs/CLEDiff/lib/python3.11/site-packages (from colorspacious) (1.24.3)\n",
      "Downloading colorspacious-1.1.2-py2.py3-none-any.whl (37 kB)\n",
      "Installing collected packages: colorspacious\n",
      "Successfully installed colorspacious-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install colorspacious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTAR E VALIDAR O RESULTADO DO LAB LOSS E LCH LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m t_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m lab_loss \u001b[38;5;241m=\u001b[39m calculate_lab_loss(gt_image, t_image)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 37\u001b[0m lch_loss \u001b[38;5;241m=\u001b[39m calculate_lch_loss(gt_image, t_image)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAB Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlab_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLCH Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlch_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mcalculate_lch_loss\u001b[0;34m(gt_image, t_image)\u001b[0m\n\u001b[1;32m     22\u001b[0m gt_lab \u001b[38;5;241m=\u001b[39m rgb_to_lab(gt_image)\n\u001b[1;32m     23\u001b[0m t_lab \u001b[38;5;241m=\u001b[39m rgb_to_lab(t_image)\n\u001b[0;32m---> 24\u001b[0m gt_lch \u001b[38;5;241m=\u001b[39m lab_to_lch(gt_lab)\n\u001b[1;32m     25\u001b[0m t_lch \u001b[38;5;241m=\u001b[39m lab_to_lch(t_lab)\n\u001b[1;32m     26\u001b[0m lch_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum((gt_lch \u001b[38;5;241m-\u001b[39m t_lch) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mlab_to_lch\u001b[0;34m(lab_image)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlab_to_lch\u001b[39m(lab_image):\n\u001b[0;32m---> 11\u001b[0m     lab_image \u001b[38;5;241m=\u001b[39m lab_image\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convertendo para (N, H, W, C) e numpy\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     lch_image \u001b[38;5;241m=\u001b[39m cs\u001b[38;5;241m.\u001b[39mcspace_convert(lab_image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIELab\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIELCh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(lch_image)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import colorspacious as cs\n",
    "\n",
    "def rgb_to_lab(image):\n",
    "    # Assumindo que a imagem está em formato (N, C, H, W)\n",
    "    image = image.permute(0, 2, 3, 1).detach().numpy()  # Convertendo para (N, H, W, C) e numpy\n",
    "    image_lab = cs.cspace_convert(image, \"sRGB1\", \"CIELab\")\n",
    "    return torch.tensor(image_lab).permute(0, 3, 1, 2).to(device)  # Convertendo de volta para tensor PyTorch\n",
    "\n",
    "def lab_to_lch(lab_image):\n",
    "    lab_image = lab_image.permute(0, 2, 3, 1).detach().numpy()  # Convertendo para (N, H, W, C) e numpy\n",
    "    lch_image = cs.cspace_convert(lab_image, \"CIELab\", \"CIELCh\")\n",
    "    return torch.tensor(lch_image).permute(0, 3, 1, 2).to(device)  # Convertendo de volta para tensor PyTorch\n",
    "\n",
    "def calculate_lab_loss(gt_image, t_image):\n",
    "    gt_lab = rgb_to_lab(gt_image)\n",
    "    t_lab = rgb_to_lab(t_image)\n",
    "    lab_loss = torch.sqrt(torch.sum((gt_lab - t_lab) ** 2, dim=1)).mean()\n",
    "    return lab_loss\n",
    "\n",
    "def calculate_lch_loss(gt_image, t_image):\n",
    "    gt_lab = rgb_to_lab(gt_image)\n",
    "    t_lab = rgb_to_lab(t_image)\n",
    "    gt_lch = lab_to_lch(gt_lab)\n",
    "    t_lch = lab_to_lch(t_lab)\n",
    "    lch_loss = torch.sqrt(torch.sum((gt_lch - t_lch) ** 2, dim=1)).mean()\n",
    "    return lch_loss\n",
    "\n",
    "# Exemplo de uso\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Exemplo de uso\n",
    "gt_image = torch.randn(1, 3, 64, 64, requires_grad=True)\n",
    "t_image = torch.randn(1, 3, 64, 64, requires_grad=True)\n",
    "\n",
    "lab_loss = calculate_lab_loss(gt_image, t_image)\n",
    "lch_loss = calculate_lch_loss(gt_image, t_image).cpu()\n",
    "\n",
    "print(f'LAB Loss: {lab_loss.item()}')\n",
    "print(f'LCH Loss: {lch_loss.item()}')\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /TESTE DE /METRICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_psnr, compare_ssim\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gusanagy/Documents/Glown-Diffusion/Metrics/metrics.py\", line 15, in <module>\n",
      "    from skimage.measure import compare_psnr, compare_ssim\n",
      "ImportError: cannot import name 'compare_psnr' from 'skimage.measure' (/home/gusanagy/anaconda3/envs/CLEDiff/lib/python3.11/site-packages/skimage/measure/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "!cd Metrics && python metrics.py \"data/UDWdata/SUIM/train_val/images\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1525\n",
      "1000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/UDWdata/SUIM/train_val/images\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Caminho para a pasta do dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m result \u001b[38;5;241m=\u001b[39m split_dataset(dataset_size, proportions, dataset_path)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagens para treino: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagens para teste: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagens para validação: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from src.split_data import split_dataset\n",
    "\n",
    "dataset_size = 1000  # Tamanho do dataset\n",
    "proportions = (0.7, 0.2, 0.1)  # Proporções para treino, teste e validação\n",
    "dataset_path = \"data/UDWdata/SUIM/train_val/images\"  # Caminho para a pasta do dataset\n",
    "\n",
    "result = split_dataset(dataset_size, proportions, dataset_path)\n",
    "\n",
    "print(f\"Imagens para treino: {result['train_size']}\")\n",
    "print(f\"Imagens para teste: {result['test_size']}\")\n",
    "print(f\"Imagens para validação: {result['val_size']}\")\n",
    "\n",
    "print(\"Endereços das imagens para treino:\")\n",
    "for img in result['train_images']:\n",
    "    print(img)\n",
    "\n",
    "print(\"Endereços das imagens para teste:\")\n",
    "for img in result['test_images']:\n",
    "    print(img)\n",
    "\n",
    "print(\"Endereços das imagens para validação:\")\n",
    "for img in result['val_images']:\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
